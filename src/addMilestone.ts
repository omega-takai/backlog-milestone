import fs from "fs";
import csv from "csv-parser";
import { config } from "dotenv";
import { parseBoolean, uniq, sleep, fetchWithRetry } from "./utils";
import { createRunLogger } from "./logger";
import {
  fetchIssueDetail,
  fetchMilestoneMap,
  patchIssueMilestones,
} from "./backlogApi";

config();

const SPACE_URL = process.env.BACKLOG_SPACE_URL!;
const PROJECT_KEY = process.env.BACKLOG_PROJECT_KEY!;
const CSV_FILE = process.env.CSV_FILE!;
const ENV_DRY_RUN = process.env.DRY_RUN || process.env.BACKLOG_DRY_RUN;
const LOG_DIR = process.env.LOG_DIR!;
const TARGET_MILESTONE = (process.env.MILESTONE || "").trim();
const SKIP_IF_MILESTONE_EXISTS = (
  process.env.SKIP_IF_MILESTONE_EXISTS || ""
).trim();
const ISSUE_KEY_COLUMN = process.env.ISSUE_KEY_COLUMN!;
const DELAY_MS = parseInt(process.env.DELAY_MS || "800");
interface CsvRow {
  [key: string]: string;
}

const CLI_DRY_RUN =
  process.argv.includes("--dry-run") && !process.argv.includes("--no-dry-run");
const DRY_RUN = CLI_DRY_RUN || parseBoolean(ENV_DRY_RUN);

const logFilePath = DRY_RUN ? `add-milestone-dry-run` : `add-milestone`;
const { logger, filePath: LOG_FILE } = createRunLogger(LOG_DIR, logFilePath);

async function addMilestoneToIssue(
  issueKey: string,
  milestoneName: string,
  milestoneMap: Record<string, number>
): Promise<void> {
  const issue = await fetchWithRetry({
    apiCall: () => fetchIssueDetail(issueKey),
    baseDelay: 0,
  });
  const { milestone: milestonesBefore = [] } = issue;
  const beforeMilestoneNames = milestonesBefore.map((m) => m.name);

  // ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ã®ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
  if (SKIP_IF_MILESTONE_EXISTS) {
    const skipMilestones = SKIP_IF_MILESTONE_EXISTS.split(",").map((m) =>
      m.trim()
    );
    const hasSkipMilestone = skipMilestones.some((skipMilestone) =>
      beforeMilestoneNames.includes(skipMilestone)
    );

    if (hasSkipMilestone) {
      logger.group(`[SKIP] ${issueKey} ${issue.summary ?? ""}`);
      logger.logDiff(beforeMilestoneNames, [], false);
      logger.groupEnd();
      return;
    }
  }

  if (!milestoneMap[milestoneName]) {
    logger.group(`[SKIP] ${issueKey} ${issue.summary ?? ""}`);
    logger.error(
      `æŒ‡å®šã®ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ãŒå­˜åœ¨ã—ã¾ã›ã‚“: \"${milestoneName}\"`,
      `ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ=${PROJECT_KEY}`
    );
    logger.groupEnd();
    return;
  }

  const afterNames = uniq([...beforeMilestoneNames, milestoneName]);
  const noChange =
    uniq(beforeMilestoneNames).sort().join("|") ===
    afterNames.slice().sort().join("|");

  const label = DRY_RUN ? "DRY-RUN" : "APPLY";
  logger.group(`[${label}] ${issueKey} ${issue.summary ?? ""}`);
  logger.logDiff(beforeMilestoneNames, afterNames, !noChange);

  if (noChange || DRY_RUN) {
    logger.groupEnd();
    return;
  }

  const milestoneIds = afterNames
    .map((name) => milestoneMap[name])
    .filter((id): id is number => Boolean(id));

  try {
    await fetchWithRetry({
      apiCall: () => patchIssueMilestones(issueKey, milestoneIds),
      baseDelay: DELAY_MS,
    });
  } catch (err: any) {
    logger.log("");
    logger.error("âŒ æ›´æ–°å¤±æ•—:", err.response?.data || err.message);
  } finally {
    logger.groupEnd();
  }
}

async function run() {
  if (!TARGET_MILESTONE) {
    logger.error(
      "MILESTONE ç’°å¢ƒå¤‰æ•°ãŒæœªæŒ‡å®šã§ã™ã€‚ä¾‹: MILESTONE=v1.0 pnpm run add-milestone"
    );
    logger.close();
    process.exit(1);
  }

  const milestoneMap = await fetchMilestoneMap();

  const rows: CsvRow[] = [];
  let rowCount = 0;
  let processedCount = 0;
  let skippedCount = 0;

  // CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§é…åˆ—ã«æ ¼ç´
  await new Promise<void>((resolve, reject) => {
    fs.createReadStream(CSV_FILE)
      .pipe(
        csv({
          mapHeaders: ({ header }) => (header ? header.trim() : header),
          mapValues: ({ value }) =>
            typeof value === "string" ? value.trim() : value,
        })
      )
      .on("data", (row: CsvRow) => {
        rows.push(row);
      })
      .on("end", resolve)
      .on("error", reject);
  });

  // é †æ¬¡å®Ÿè¡Œã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å›é¿
  for (let i = 0; i < rows.length; i++) {
    const row = rows[i];
    rowCount += 1;
    const issueKey = row[ISSUE_KEY_COLUMN];

    if (!issueKey) {
      skippedCount += 1;
      logger.log(
        `row#${rowCount}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ${ISSUE_KEY_COLUMN} æ¬„ãŒç©ºï¼‰ issueIdOrKey=(none)`
      );
      continue;
    }

    processedCount += 1;
    logger.log(`\n[${processedCount}/${rows.length}] å‡¦ç†ä¸­: ${issueKey}`);

    try {
      await addMilestoneToIssue(issueKey, TARGET_MILESTONE, milestoneMap);

      // ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ã®ãŸã‚ã€APIå‘¼ã³å‡ºã—é–“ã«800mså¾…æ©Ÿ
      // Backlog APIã¯1åˆ†é–“ã«60ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§ãªã®ã§ã€800msé–“éš”ã§å®‰å…¨ï¼ˆç†è«–å€¤1000msã‹ã‚‰20%å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã‚’å¼•ã„ãŸå€¤ï¼‰
      if (i < rows.length - 1) {
        await sleep(800);
      }
    } catch (error: any) {
      logger.error(`èª²é¡Œ ${issueKey} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼:`, error?.message || error);
    }
  }

  logger.log(
    `\nğŸ‰ å…¨èª²é¡Œã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ (rows=${rowCount}, processed=${processedCount}, skipped=${skippedCount})`
  );
  logger.close();
}

logger.log(`Log file: ${LOG_FILE}`);
logger.log(`Space: ${SPACE_URL}, Project: ${PROJECT_KEY}`);
logger.log(`CSV: ${CSV_FILE}`);
logger.log(`Mode: ${DRY_RUN ? "DRY-RUN" : "APPLY"}`);
logger.log(`Target Milestone: ${TARGET_MILESTONE || "(none)"}`);
logger.log(`Skip if milestone exists: ${SKIP_IF_MILESTONE_EXISTS || "(none)"}`);

run();
