import fs from "fs";
import csv from "csv-parser";
import { createRunLogger } from "./logger";
import {
  fetchMilestoneMap,
  fetchIssueDetail,
  patchIssueMilestones,
} from "./backlogApi";
import { config } from "dotenv";
import { parseBoolean, uniq, sleep, fetchWithRetry } from "./utils";

config();

const SPACE_URL = process.env.BACKLOG_SPACE_URL!;
const PROJECT_KEY = process.env.BACKLOG_PROJECT_KEY!;
const CSV_FILE = process.env.CSV_FILE!;
const ENV_DRY_RUN = process.env.DRY_RUN || process.env.BACKLOG_DRY_RUN;
const LOG_DIR = process.env.LOG_DIR!;
const SKIP_IF_MILESTONE_EXISTS = (
  process.env.SKIP_IF_MILESTONE_EXISTS || ""
).trim();
const ISSUE_KEY_COLUMN = process.env.ISSUE_KEY_COLUMN!;
const MILESTONE_COLUMN = process.env.MILESTONE_COLUMN!;
const DELAY_MS = parseInt(process.env.DELAY_MS || "800");

interface CsvRow {
  [key: string]: string;
}

const CLI_DRY_RUN =
  process.argv.includes("--dry-run") && !process.argv.includes("--no-dry-run");
const DRY_RUN = CLI_DRY_RUN || parseBoolean(ENV_DRY_RUN);

const logFilePath = DRY_RUN ? `update-dry-run` : `update`;
const { logger, filePath: LOG_FILE } = createRunLogger(LOG_DIR, logFilePath);

// 1. èª²é¡Œã‚’æ›´æ–°ï¼ˆDryRunå¯¾å¿œï¼†Before/Afterãƒ­ã‚°ï¼‰
async function updateIssue(
  issueKey: string,
  desiredMilestoneNames: string[],
  milestoneMap: Record<string, number>
): Promise<void> {
  const issue = await fetchWithRetry({
    apiCall: () => fetchIssueDetail(issueKey),
    baseDelay: 0,
  });
  const { milestone: milestonesBefore = [] } = issue;
  const beforeMilestoneNames = milestonesBefore.map((m) => m.name);

  // ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ã®ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
  if (SKIP_IF_MILESTONE_EXISTS) {
    const skipMilestones = SKIP_IF_MILESTONE_EXISTS.split(",").map((m) =>
      m.trim()
    );
    const hasSkipMilestone = skipMilestones.some((skipMilestone) =>
      beforeMilestoneNames.includes(skipMilestone)
    );

    if (hasSkipMilestone) {
      logger.group(`[SKIP] ${issueKey} ${issue.summary ?? ""}`);
      logger.logDiff(beforeMilestoneNames, [], false);
      logger.groupEnd();
      return;
    }
  }

  // CSVï¼‹è‡ªå‹•ä»˜ä¸ã‹ã‚‰Afterã‚’ä½œã‚‹ï¼ˆæœ‰åŠ¹ãªãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³åã®ã¿ã€é‡è¤‡æ’é™¤ã€trimï¼‰
  const normalizedDesired = desiredMilestoneNames
    .map((n) => n.trim())
    .filter((n) => n.length > 0);
  const validDesired = normalizedDesired.filter((name) =>
    Boolean(milestoneMap[name])
  );
  const afterNames = uniq(validDesired);

  const noChange =
    uniq(beforeMilestoneNames).sort().join("|") ===
    afterNames.slice().sort().join("|");

  const label = DRY_RUN ? "DRY-RUN" : "APPLY";
  logger.group(`[${label}] ${issueKey} ${issue.summary ?? ""}`);
  logger.logDiff(beforeMilestoneNames, afterNames, !noChange);

  if (noChange || DRY_RUN) {
    logger.groupEnd();
    return;
  }

  const milestoneIds = afterNames
    .map((name) => milestoneMap[name])
    .filter((id): id is number => Boolean(id));

  try {
    await fetchWithRetry({
      apiCall: () => patchIssueMilestones(issueKey, milestoneIds),
      baseDelay: DELAY_MS,
    });
  } catch (err: any) {
    logger.log("");
    logger.error("âŒ æ›´æ–°å¤±æ•—:", err.response?.data || err.message);
  } finally {
    logger.groupEnd();
  }
}

// 2. CSVã‚’èª­ã¿è¾¼ã‚“ã§å‡¦ç†
async function run() {
  const milestoneMap = await fetchMilestoneMap();

  const rows: CsvRow[] = [];
  let rowCount = 0;
  let processedCount = 0;
  let skippedCount = 0;

  // CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§é…åˆ—ã«æ ¼ç´
  await new Promise<void>((resolve, reject) => {
    fs.createReadStream(CSV_FILE)
      .pipe(
        csv({
          mapHeaders: ({ header }) => (header ? header.trim() : header),
          mapValues: ({ value }) =>
            typeof value === "string" ? value.trim() : value,
        })
      )
      .on("data", (row: CsvRow) => {
        rows.push(row);
      })
      .on("end", resolve)
      .on("error", reject);
  });

  // é †æ¬¡å®Ÿè¡Œã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å›é¿
  for (let i = 0; i < rows.length; i++) {
    const row = rows[i];
    rowCount += 1;
    const issueKey = row[ISSUE_KEY_COLUMN];
    const rawMilestone = row[MILESTONE_COLUMN] || "";
    const milestoneNames = rawMilestone
      ? rawMilestone.split(",").map((s) => s.trim())
      : [];

    if (!issueKey) {
      skippedCount += 1;
      logger.log(
        `row#${rowCount}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ${ISSUE_KEY_COLUMN} æ¬„ãŒç©ºï¼‰ issueIdOrKey=(none)`
      );
      continue;
    }

    processedCount += 1;
    logger.log(`\n[${processedCount}/${rows.length}] å‡¦ç†ä¸­: ${issueKey}`);

    try {
      await updateIssue(issueKey, milestoneNames, milestoneMap);

      // ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ã®ãŸã‚ã€APIå‘¼ã³å‡ºã—é–“ã«å¾…æ©Ÿ
      // Backlog APIã¯1åˆ†é–“ã«60ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§ãªã®ã§ã€DELAY_MSé–“éš”ã§å®‰å…¨
      if (i < rows.length - 1) {
        await sleep(DELAY_MS);
      }
    } catch (error: any) {
      logger.error(`èª²é¡Œ ${issueKey} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼:`, error?.message || error);
    }
  }

  logger.log(
    `\nğŸ‰ å…¨èª²é¡Œã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ (rows=${rowCount}, processed=${processedCount}, skipped=${skippedCount})`
  );
  logger.close();
}

// å®Ÿè¡Œé–‹å§‹ãƒ˜ãƒƒãƒ€ï¼ˆrunå‰ã«å‡ºåŠ›ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã¸ç¢ºå®Ÿã«æ›¸ã‹ã‚Œã‚‹ã‚ˆã†å…ˆå‡ºã—ï¼‰
logger.log(`Log file: ${LOG_FILE}`);
logger.log(`Space: ${SPACE_URL}, Project: ${PROJECT_KEY}`);
logger.log(`CSV: ${CSV_FILE}`);
logger.log(`Mode: ${DRY_RUN ? "DRY-RUN" : "APPLY"}`);
logger.log(`Skip if milestone exists: ${SKIP_IF_MILESTONE_EXISTS || "(none)"}`);
logger.log(`Delay: ${DELAY_MS}ms`);

run();
